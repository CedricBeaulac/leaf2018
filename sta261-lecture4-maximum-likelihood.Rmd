---
title: "STA261 Summer 2018 Lecture 4:" 
subtitle: "Maximum Likelihood"
author: "Alex Stringer"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup-noshow, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r setup-show,include=TRUE}
# Load the tidyverse packages
suppressMessages({
  suppressWarnings({
    library(tidyverse)
  })
})
```

# Likelihood

Recall in lecture how we defined the *likelihood* function: for a random vector $X = (X_{1},\ldots,X_{n})$ whose joint distribution depends on parameter $\theta = (\theta_{1},\ldots,\theta_{p})$, the likelihood function is the joint density/mass function of $X$ treated as a function of $\theta$ for fixed $X$:
\[
L(\theta | X) = f(x ; \theta)
\]
The most common case is where $X$ represents an IID random sample of a single random variable; the joint density and hence the likelihood is a product of the marginal densities of $X_{i}$, which are all the same:
\[
L(\theta|X) = \prod_{i=1}^{n}f(x_{i};\theta)
\]
For computational and theoretical reasons, we said we would work primarily with the *log-likelihood*,
\[
\ell(\theta) = \log{L(\theta)} \overset{IID}{=} \sum_{i=1}^{n}\ell_{i}(\theta)
\]
What does it mean, "for fixed $X$"? **Every time we take a random sample $x = (x_{1},\ldots,x_{n})$, we get a different log-likelihood function**. This is the point: the likelihood describes the relative frequency with which each value of $\theta$ would have generated the observed data, in repeated sampling from a population with true parameter eqaul to $\theta$. Values of $\theta$ that give higher likelihoods would generate the observed data more frequently- and hence are said to be *more likely* to have generated the observed data.

This suggests a framework for estimation. If we have a family of probability distributions in mind (we generally get this from subject-matter theory or by assessing the data visually; see examples), then we should *find the value(s) of the parameter(s) of this family of distributions that maximize the likelihood function- this/these is/are the value(s) most likely to have generated the sample that we observed*. This is the method of **maximum likelihood**.

# Examples

## Normal Distribution

Consider the dataset from Rice (2006), page 379, on the melting point in degrees celcius of beeswax. As described by Rice, the aim of the study was to detect synthetic additions to pure beeswax, and the science behind this dictates that the melting point is one way to measure this. It was of scientific interest to fit a probability model to these data. How to proceed?

We can read in the data as follows:
```{r beeswax-1}
path <- "https://raw.githubusercontent.com/awstringer1/leaf2018/gh-pages/datasets/"
beeswax <- readr::read_csv(stringr::str_c(path,"beeswax.txt"))
glimpse(beeswax)

```
We see the dataset has two variables; we are interested in `MeltingPoint`. Let's make a histogram to investigate the shape of the distribution of this variable:
```{r beeswax-2}
beeswax %>%
  ggplot(aes(x = MeltingPoint)) +
  theme_classic() +
  geom_histogram(bins = 25,colour = "black",fill = "red") +
  labs(title = "Histogram of Beeswax Melting Points",
       subtitle = "Beeswax data",
       x = "Melting Point (degrees celcius",
       y = "# of Samples")

```
It is hard to tell visually if a Gaussian curve would be a good fit, since the histogram is not very smooth due to the low sample size.

Recall the Gaussian distribution has density given by
\[
f(x;\mu,\sigma^{2}) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\times\exp\left( -\frac{1}{2\sigma^{2}}\left( x - \mu\right)^{2}\right)
\]
which for an IID random sample of size $n$ gives log-likelihood
\[
\ell(\mu,\sigma^{2}) = -\frac{n}{2}\log\left( 2\pi\sigma^{2}\right) -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left( x_{i} - \mu\right)^{2}
\]
Let's plot this for the beeswas data as a function of $\mu$ for fixed $\sigma^{2}$ and vice-versa:
