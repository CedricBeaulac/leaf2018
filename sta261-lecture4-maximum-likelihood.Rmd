---
title: "STA261 Summer 2018 Lecture 4:" 
subtitle: "Maximum Likelihood"
author: "Alex Stringer"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup-noshow, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r setup-show,include=TRUE}
# Load the tidyverse packages
suppressMessages({
  suppressWarnings({
    library(tidyverse)
  })
})
```

# Likelihood

Recall in lecture how we defined the *likelihood* function: for a random vector $X = (X_{1},\ldots,X_{n})$ whose joint distribution depends on parameter $\theta = (\theta_{1},\ldots,\theta_{p})$, the likelihood function is the joint density/mass function of $X$ treated as a function of $\theta$ for fixed $X$:
\[
L(\theta | X) = f(x ; \theta)
\]
The most common case is where $X$ represents an IID random sample of a single random variable; the joint density and hence the likelihood is a product of the marginal densities of $X_{i}$, which are all the same:
\[
L(\theta|X) = \prod_{i=1}^{n}f(x_{i};\theta)
\]
For computational and theoretical reasons, we said we would work primarily with the *log-likelihood*,
\[
\ell(\theta) = \log{L(\theta)} \overset{IID}{=} \sum_{i=1}^{n}\ell_{i}(\theta)
\]
What does it mean, "for fixed $X$"? **Every time we take a random sample $x = (x_{1},\ldots,x_{n})$, we get a different log-likelihood function**. This is the point: the likelihood describes the relative frequency with which each value of $\theta$ would have generated the observed data, in repeated sampling from a population with true parameter eqaul to $\theta$. Values of $\theta$ that give higher likelihoods would generate the observed data more frequently- and hence are said to be *more likely* to have generated the observed data.

This suggests a framework for estimation. If we have a family of probability distributions in mind (we generally get this from subject-matter theory or by assessing the data visually; see examples), then we should *find the value(s) of the parameter(s) of this family of distributions that maximize the likelihood function- this/these is/are the value(s) most likely to have generated the sample that we observed*. This is the method of **maximum likelihood**.

# Examples

## Normal Distribution

Consider the dataset from Rice (2006), page 258, on fluctuations in current across a muscle cell membrane containing a large number of independently operating channels. Measurements from each of the $49,152$ channels were obtained, and it was of scientific interest to fit a probability distribution to the data; estimated parameters from this fit would be used then to derive properties of scientific interest.

How to proceed? We can read in the data as follows:
