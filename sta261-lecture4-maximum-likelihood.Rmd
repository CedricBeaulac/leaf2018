---
title: "STA261 Summer 2018 Lecture 4:" 
subtitle: "Maximum Likelihood"
author: "Alex Stringer"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup-noshow, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r setup-show,include=TRUE}
# Load the tidyverse packages
suppressMessages({
  suppressWarnings({
    library(tidyverse)
  })
})
```

# Likelihood

Recall in lecture how we defined the *likelihood* function: for a random vector $X = (X_{1},\ldots,X_{n})$ whose joint distribution depends on parameter $\theta = (\theta_{1},\ldots,\theta_{p})$, the likelihood function is the joint density/mass function of $X$ treated as a function of $\theta$ for fixed $X$:
\[
L(\theta | X) = f(x ; \theta)
\]
The most common case is where $X$ represents an IID random sample of a single random variable; the joint density and hence the likelihood is a product of the marginal densities of $X_{i}$, which are all the same:
\[
L(\theta|X) = \prod_{i=1}^{n}f(x_{i};\theta)
\]
For computational and theoretical reasons, we said we would work primarily with the *log-likelihood*,
\[
\ell(\theta) = \log{L(\theta)} \overset{IID}{=} \sum_{i=1}^{n}\ell_{i}(\theta)
\]
What does it mean, "for fixed $X$"? **Every time we take a random sample $x = (x_{1},\ldots,x_{n})$, we get a different log-likelihood function**. This is the point: the likelihood describes the relative frequency with which each value of $\theta$ would have generated the observed data, in repeated sampling from a population with true parameter eqaul to $\theta$. Values of $\theta$ that give higher likelihoods would generate the observed data more frequently- and hence are said to be *more likely* to have generated the observed data.

This suggests a framework for estimation. If we have a family of probability distributions in mind (we generally get this from subject-matter theory or by assessing the data visually; see examples), then we should *find the value(s) of the parameter(s) of this family of distributions that maximize the likelihood function- this/these is/are the value(s) most likely to have generated the sample that we observed*. This is the method of **maximum likelihood**.

# Examples

## Normal Distribution

Consider the dataset from Rice (2006), page 379, on the melting point in degrees celcius of beeswax. As described by Rice, the aim of the study was to detect synthetic additions to pure beeswax, and the science behind this dictates that the melting point is one way to measure this. It was of scientific interest to fit a probability model to these data. How to proceed?

We can read in the data as follows:
```{r beeswax-1}
path <- "https://raw.githubusercontent.com/awstringer1/leaf2018/gh-pages/datasets/"
beeswax <- readr::read_csv(stringr::str_c(path,"beeswax.txt"))
glimpse(beeswax)

```
We see the dataset has two variables; we are interested in `MeltingPoint`. Let's make a histogram to investigate the shape of the distribution of this variable:
```{r beeswax-2}
beeswax %>%
  ggplot(aes(x = MeltingPoint)) +
  theme_classic() +
  geom_histogram(bins = 25,colour = "black",fill = "red") +
  labs(title = "Histogram of Beeswax Melting Points",
       subtitle = "Beeswax data",
       x = "Melting Point (degrees celcius",
       y = "# of Samples")

```
It is hard to tell visually if a Gaussian curve would be a good fit, since the histogram is not very smooth due to the low sample size.

Recall the Gaussian distribution has density given by
\[
f(x;\mu,\sigma^{2}) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\times\exp\left( -\frac{1}{2\sigma^{2}}\left( x - \mu\right)^{2}\right)
\]
which for an IID random sample of size $n$ gives log-likelihood
\[
\ell(\mu,\sigma^{2}) = -\frac{n}{2}\log\left( 2\pi\sigma^{2}\right) -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left( x_{i} - \mu\right)^{2}
\]
Let's plot this for the beeswax data as a function of $\mu$ for fixed $\sigma^{2}$ and vice-versa.
```{r beeswax-3}
ll_mu <- function(mu) {
  x <- beeswax$MeltingPoint
  s2 <- var(x)
  n <- length(x)
  out <- numeric(length(mu))
  for (i in 1:length(mu)) {
    out[i] <- -(n/2) * log(2*pi*s2) - (1/(2*s2)) * sum((x - mu[i])^2)
  }
  out
}
ll_sigma <- function(sigmasq) {
  x <- beeswax$MeltingPoint
  xbar <- mean(x)
  n <- length(x)
  out <- numeric(length(sigmasq))
  for (i in 1:length(sigmasq)) {
    out[i] <- -(n/2) * log(2*pi*sigmasq[i]) - (1/(2*sigmasq[i])) * sum((x - xbar)^2)
  }
  out
}


beeswax_muplot <-  data_frame(mu = seq(62.2,65,by=0.01),
                              ll = ll_mu(mu)) %>%
  ggplot(aes(x = mu,y = ll,group = 1)) +
  theme_classic() + 
  geom_line() +
  geom_vline(xintercept = mean(beeswax$MeltingPoint),colour="purple") +
  labs(title = "Log-Likelihood for mu, Beeswax Melting Point Data",
       subtitle = "Sigma fixed at MLE. Purple line indictaes MLE for mu",
       x = "Mu",
       y = "Log-Likelihood") +
  scale_x_continuous(breaks = seq(62,65,by=0.5))


beeswax_sigmaplot <-  data_frame(sigmasq = seq(0.01,.5,by=0.01),
                              ll = ll_sigma(sigmasq)) %>%
  ggplot(aes(x = sigmasq,y = ll,group = 1)) +
  theme_classic() + 
  geom_line() +
  geom_vline(xintercept = var(beeswax$MeltingPoint),colour="purple") +
  labs(title = "Log-Likelihood for sigma-squared, Beeswax Melting Point Data",
       subtitle = "Mu fixed at MLE. Purple line indicates MLE for sigma-squared",
       x = "Sigma-Squared",
       y = "Log-Likelihood")

cowplot::plot_grid(beeswax_muplot,beeswax_sigmaplot,nrow=1)
```
We see some interesting things:

  - The log-likelihood for $\mu$ is of the same shape as the density of $x_{i}$. This makes sense, because both are functions of $x_{i}$ and $\mu$ only through the $\sum_{i=1}^{n}(x_{i} - \mu)^{2}$ term, which is symmetric in both those arguments
  - The MLE for $\mu$ is clearly at the centre of this plot, and equal to `r round(mean(beeswax$MeltingPoint),2)`- this is exactly the centre of the histogram of the data. Values of $\mu$ that are away from the maximum in either direction are equally less likely to have generated the observed data
  - The situation for $\sigma^{2}$ is completely different. The MLE is the sample variance (as seen in lecture) as indicated by the purple line. Values very near the MLE are nearly exactly as likely to have generated the observed data as the MLE- we lack *precision* in estimating this parameter. We see also that values that are less than the MLE quickly become very unlikely, while values much larger than the MLE are still pretty likely. Think about what these values represent in the context of the data that was observed. We got a sample mean of `r round(mean(beeswax$MeltingPoint),2)` and a range of datapoints of `r round(range(beeswax$MeltingPoint),2)`. If the population variance were very small, say $0.02$, then seeing a value of $62.85$, say, would be extremely improbable, as this value would be over $5$ standard deviations from the mean. However, if the population variance were very large, say $0.5$, then all the values we have observed are actually quite close to the mean! Hence larger values of the population variance are much more likely to have generated the data we observed than small values.
  
Let's investigate further what different values of the parameters mean in the context of the data we have observed. Fixing $\sigma^{2}$ at its MLE $s_{n}^{2}$, we overlay a Normal density curve on the histogram of observed data, for $\mu$ at its MLE, $\bar{x}$:
